import sys
import os
from dotenv import load_dotenv

from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory
from langchain_community.vectorstores import FAISS
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import (
    create_history_aware_retriever,
    create_retrieval_chain
)

from utils.model_loader import ModelLoader
from exception.custom_exception import DocumentPortalException
from logger.custom_logger import CustomLogger
from prompt.prompt_library import PROMPT_REGISTRY
from models.models import PromptType

# In-memory session store for chat history
_SESSION_STORE = {}


class ConversationalRAG:
    def __init__(self, session_id: str, retriever):
        try:
            self.log = CustomLogger.get_logger(__name__)
            self.session_id = session_id
            self.retriever = retriever

            # Load LLM
            self.llm = self._load_llm()

            # Load prompts
            self.contextualize_prompt = PROMPT_REGISTRY[
                PromptType.CONTEXTUALIZE_QUESTION.value
            ]
            self.qa_prompt = PROMPT_REGISTRY[
                PromptType.CONTEXT_QA.value
            ]

            # History-aware retriever
            self.history_aware_retriever = create_history_aware_retriever(
                self.llm,
                self.retriever,
                self.contextualize_prompt
            )

            self.log.info(
                "Created history-aware retriever",
                session_id=self.session_id
            )

            # QA + RAG chains
            self.qa_chain = create_stuff_documents_chain(
                self.llm,
                self.qa_prompt
            )

            self.rag_chain = create_retrieval_chain(
                self.history_aware_retriever,
                self.qa_chain
            )

            self.log.info(
                "Created RAG chain",
                session_id=self.session_id
            )

            # Runnable with message history
            self.chain = RunnableWithMessageHistory(
                self.rag_chain,
                self._get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer"
            )

        except Exception as e:
            self.log.error(
                "Failed to initialize ConversationalRAG",
                error=str(e),
                session_id=self.session_id
            )
            raise DocumentPortalException(
                "Initialization error in ConversationalRAG",
                sys
            )

    def _load_llm(self):
        try:
            model_loader = ModelLoader()
            llm = model_loader.load_llm()
            self.log.info(
                "LLM loaded",
                class_name=llm.__class__.__name__
            )
            return llm
        except Exception as e:
            self.log.error(
                "Failed to load LLM",
                error=str(e)
            )
            raise DocumentPortalException(
                "Failed to load LLM",
                sys
            )

    def _get_session_history(self,session_id: str) -> BaseChatMessageHistory:

    
        if session_id not in _SESSION_STORE:

            _SESSION_STORE[session_id] = InMemoryChatMessageHistory()


        return _SESSION_STORE[session_id]


    def load_retriever_from_faiss(self, index_path: str):
        try:
            model_loader = ModelLoader()
            embeddings = model_loader.load_embedding_model()

            if not os.path.isdir(index_path):
                raise FileNotFoundError(
                    f"FAISS index not found at {index_path}"
                )

            vectorstore = FAISS.load_local(
                index_path,
                embeddings,
                allow_dangerous_deserialization=True
            )

            self.log.info(
                "Loaded retriever from FAISS index",
                index_path=index_path
            )

            return vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            )

        except Exception as e:
            self.log.error(
                "Failed to load retriever",
                error=str(e)
            )
            raise DocumentPortalException(
                "Failed to load retriever",
                sys
            )

    def invoke(self, user_input: str) -> str:
        try:
            response = self.chain.invoke(
                {"input": user_input},
                config={
                    "configurable": {
                        "session_id": self.session_id
                    }
                }
            )

            answer = response.get("answer", "No Answer")

            if not answer:
                self.log.warning(
                    "No answer found in response",
                    session_id=self.session_id
                )

            self.log.info(
                "Chain invoked successfully",
                session_id=self.session_id,
                user_input=user_input,
                answer_preview=answer[:150]
            )

            return answer

        except Exception as e:
            self.log.error(
                "Failed to invoke ConversationalRAG",
                error=str(e),
                session_id=self.session_id
            )
            raise DocumentPortalException(
                "Failed to invoke ConversationalRAG",
                sys
            )
